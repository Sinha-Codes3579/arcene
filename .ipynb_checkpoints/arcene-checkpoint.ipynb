import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from harmony_search import initialize_harmony_memory, evaluate_fitness
from harmony_search import improvise_new_harmony, update_harmony_memory, accuracy_hs, feature_counts_hs
from pso import initialize_particles, binary_pso
from final_model import final_evaluation
from visualization import plot_accuracy_over_time, plot_features_selected, plot_confusion_matrix, plot_accuracy_vs_reduction
from sklearn.svm import SVC

# Load datasets
def load_arcene_dataset(path="./ARCENE/"):
    # training data
    X_train = np.loadtxt(path + "arcene_train.data")
    y_train = np.loadtxt(path + "arcene_train.labels")

    # testing data
    X_test = np.loadtxt(path + "arcene_valid.data")
    y_test = np.loadtxt(path + "arcene_valid.labels")

    print(f"Training shape: {X_train.shape}, Test shape: {X_test.shape}")
    print(f"Train Labels: {np.unique(y_train)}, Test Labels: {np.unique(y_test)}")
    return X_train, y_train, X_test, y_test

# Preprocessing the dataset
def preprocess_data(X_train, X_test, scaler_type="standard"):
    if scaler_type == "minmax":
        scaler = MinMaxScaler()
    else:
        scaler = StandardScaler()

    # fit on training data and transform both sets
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled

# check missing values
def check_missing(X_train, X_test):
    print("Checking missing values:")
    print(f"Train missing: {np.isnan(X_train).sum()}, Test missing: {np.isnan(X_test).sum()}")

# combine rain and test
def combine_datasets(X_train, y_train, X_test, y_test):
    X_combined = np.vstack((X_train, X_test))
    y_combined = np.hstack((y_train, y_test))
    return X_combined, y_combined

if __name__ == "__main__":
    X_train, y_train, X_test, y_test = load_arcene_dataset()
    # preprocess
    X_train_scaled, X_test_scaled = preprocess_data(X_train, X_test, scaler_type="standard")
    # check missing values
    check_missing(X_train_scaled, X_test_scaled)
    # combine for cross-validation use
    X_combined, y_combined = combine_datasets(X_train_scaled, y_train, X_test_scaled, y_test)
    print(f"Combined shape:{X_combined.shape}, Combined labels: {np.unique(y_combined)}")



# Initialize harmony memory
HM = initialize_harmony_memory()
# evaluate all the harmonies
fitness_scores = []
for harmony in HM:
    fitness = evaluate_fitness(harmony, X_combined, y_combined)
    fitness_scores.append(fitness)

# top 3 harmonies
best_indices = np.argsort(fitness_scores)[:3]
print("Top 3 Harmony Fitness Scores: ")
for i in best_indices:
    print(f"Harmony {i} | Fitness: {fitness_scores[i]:.4f} | Features selected: {np.sum(HM[i])}")


# Harmony search exploration phase
ITERATIONS = 20
accuracy_hs = []
feature_counts_hs = []
print("\n Starting Harmony Search Exploration Phase")
for iteration in range(ITERATIONS):
    new_harmony = improvise_new_harmony(HM)
    new_fitness = evaluate_fitness(new_harmony, X_combined, y_combined)
    replaced = update_harmony_memory(HM, fitness_scores, new_harmony, new_fitness)
    acc = 1 - new_fitness  # because fitness = 1 - acc
    accuracy_hs.append(acc)
    feature_counts_hs.append(np.sum(new_harmony))
    print(f"Iteration {iteration + 1: 02} | Fitness: {new_fitness:.4f} | Replaced: {'Right' if replaced else 'Wrong'}")

# best result after HS phase
best_index = np.argmin(fitness_scores)
print(f"\n Best Fitness After HS: {fitness_scores[best_index]:.4f}")
print(f"Features Selected: {np.sum(HM[best_index])}")


# PSO
print("\n Starting PSO Exploitation Phase")
accuracy_pso = []
feature_counts_pso = []
# Use harmony memory from HS as starting particles
particles, velocities = initialize_particles(HM)

# Run Binary PSO
gbest, gbest_fitness, accuracy_pso, feature_counts_pso = binary_pso(particles, velocities, X_combined, y_combined)

print(f"\n Final Best Fitness: {gbest_fitness:.4f}")
print(f" Final Features Selected: {np.sum(gbest)}")



# evaluate final result
final_evaluation(gbest, X_train, y_train, X_test, y_test)


# Plot learning curves
plot_accuracy_over_time(accuracy_hs, accuracy_pso)
plot_features_selected(feature_counts_hs, feature_counts_pso)

# Predict final output
final_clf = SVC(kernel='linear', C=1)
X_train_final = X_train[:, gbest == 1]
X_test_final = X_test[:, gbest == 1]
final_clf.fit(X_train_final, y_train)
y_pred = final_clf.predict(X_test_final)

# Confusion Matrix
plot_confusion_matrix(y_test, y_pred)

# Bar Chart
acc_list = [accuracy_hs[-1], accuracy_pso[-1]]
red_list = [
    100 * (1 - feature_counts_hs[-1] / X_train.shape[1]),
    100 * (1 - feature_counts_pso[-1] / X_train.shape[1])
]
plot_accuracy_vs_reduction(acc_list, red_list, labels=["HS", "PSO"])